{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfb2f7d-b54d-4d2a-aaf7-c5a8b4eef5d9",
   "metadata": {},
   "source": [
    "在深度学习中，原数据集往往很大，不可能一次性的全部载入模型，训练模型都是小批量小批量地优化训练的，那么 PyTorch 是如何批量地加载数据的呢？\n",
    "\n",
    "PyTorch 的 torch.utils.data 包中定义了两个类 Dataset 和 DataLoader，通过学习这两个类，我们就会明白 PyTorch 是如何批量地加载数据的。\n",
    "\n",
    "<br/>\n",
    "\n",
    "# 1. 分割数据集\n",
    " \n",
    "在使用 Dataset 和 DataLoader 处理数据之前，需要将数据划分为训练集、验证集和测试集。\n",
    "\n",
    "详细过程请参考 `./1_split.py`\n",
    "\n",
    "<br/>\n",
    "\n",
    "# 2. Dataset（我的理解：Dataset 子类用于将数据整理为一定格式的数据）\n",
    "\n",
    "Pytorch 中给定的 Dataset 类是一个抽象类，只能用于继承，我们自定义的 Dataset 子类需要继承它，并且重写 `__getitem__()`；\n",
    "\n",
    "在自定义的 Dataset 子类中，`__getitem__()` 接收索引，返回数据，接收的索引 index 由 DataLoader() 提供，返回的数据由 Dataset 子类中的定义的方法整理好。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**2.1 下面根据人民币二分类任务中自定义的 RMBDataset 子类的代码为例介绍自定义 Dataset 子类的框架：**\n",
    "\n",
    "完整代码请查看 ../tools/my_dataset.py 中的 RMBDataset()\n",
    "\n",
    "```\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class RMBDataset(Dataset):\n",
    "    # 初始化\n",
    "    def __init__(self, data_dir):\n",
    "        self.label_name = {\"1\": 0, \"100\": 1}\n",
    "        self.data_info = self.get_img_info(data_dir)\n",
    "\n",
    "    # 定义 __getitem__()，接收索引，根据索引获取 data_info 中的数据，返回图片和 label 标签\n",
    "    def __getitem__(self, index):\n",
    "        path_img, label = self.data_info[index]\n",
    "        img = Image.open(path_img).convert('RGB')\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    # 定义 get_img_info()，输入数据路径 data_dir，返回 data_info，data_info 是一个由元组组成的列表，每个元组包含图片路径和 label 标签两个元素\n",
    "    @staticmethod\n",
    "    def get_img_info(data_dir):\n",
    "        data_info = list()\n",
    "        path_img = ···\n",
    "        label = ···\n",
    "        data_info.append((path_img, int(label)))\n",
    "        return data_info\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "**2.2 Dataset 子类的数据逻辑：**\n",
    "\n",
    "data_dir ——> get_img_info(data_dir) ——> return data_info ——> `__getitem__(index)` ——> return img, label\n",
    "\n",
    "<br/>\n",
    "\n",
    "# 3. DataLoader（我的理解：DataLoader 用于按照一定的规则加载数据）\n",
    "\n",
    "DataLoader() 用于构建可迭代的数据装载器，使用时需要实例化。\n",
    "\n",
    "**3.1 四个主要参数：**\n",
    "\n",
    "batch_size：批大小，默认值为 1\n",
    "\n",
    "num_works：读取数据的进程数，默认值为 0，0 means that the data will be loaded in the main process. \n",
    "\n",
    "shuffle：每个 epoch 是否打乱顺序，默认值为 Flase\n",
    "\n",
    "drop_last：当样本数不能被 batchsize 整除时，是否舍弃最后一批数据，默认值为 Flase\n",
    "\n",
    "<br/>\n",
    "\n",
    "**3.2 Epoch Iteration Batchsize 之间的关系：**\n",
    "\n",
    "Epoch：一个 Epoch 指的是整个训练数据集被完整地用来训练一次；\n",
    "\n",
    "Batchsize：是指每次传递给模型进行训练的数据样本数；\n",
    "\n",
    "Iteration：是指在训练过程中模型权重更新的总次数；\n",
    "\n",
    "例如：如果一个数据集中有 2000 张图片，设置 Batchsize 为 500，则一个 Epoch 中的 Iteration 数为 2000/500 = 4，这意味着跑完一个 Epoch 需要迭代 4 个 Batch。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**3.3 下面根据训练模型的数据准备部分的代码为例介绍 RMBDataset() 实例化和使用 DataLoader() 的框架：**\n",
    "\n",
    "完整代码请查看 ./2_train_lenet.py 中的数据准备部分\n",
    "\n",
    "```\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dir = ···\n",
    "\n",
    "# 构建 RMBDataset() 实例\n",
    "train_data = RMBDataset(data_dir=train_dir)\n",
    "\n",
    "# DataLoder() 实例化，构建数据加载器\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "```\n",
    "\n",
    "**1) RMBDataset 子类实例化的数据逻辑**\n",
    "\n",
    "`train_data = RMBDataset(data_dir=train_dir)`\n",
    "\n",
    "train_data：RMBDataset() 的 `__getitem__(index)` 根据 index 读取 data_info 数据后返回图片和 label 标签，然后赋值给 train_data；\n",
    "\n",
    "**2) `__getitem__(index)` 的 index 哪里来呢？**\n",
    "\n",
    "`train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)`\n",
    "\n",
    "DataLoader() 会根据 batch_size 和 num_works 的值调用采样器 samper() 返回 index 数据。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**3.4 DataLoader 的数据逻辑：**\n",
    "\n",
    "根据 batch_size 和 num_works 的值 ——> index ——> `__getitem__(index)` ——> return img, label ——> train_data ——> train_loader（多 batch 数据）\n",
    "\n",
    "train_loader 包含多 batch 数据，多 batch 数据存放在一个列表中，每一批数据都是一个包含两个元素的列表，一个元素是图片，另外一个元素是 label 标签，图片和标签也是以列表的形式存放的，所以数据形状为`[[[图片1], [标签1]], [[图片2], [标签2]], [[图片3], [标签3]]···]`。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**3.5 我的理解：**\n",
    "Dataset 子类用于将数据整理为一定格式的数据，DataLoader 用于按照一定的规则加载数据。\n",
    "\n",
    "<br/>\n",
    "\n",
    "# 5. 迭代训练模型\n",
    "\n",
    "**框架：**\n",
    "```\n",
    "for epoch in range(MAX_EPOCH):\n",
    "\n",
    "    for i, data in enumerate(train_loader):  # 循环地从 train_loader 中获取一个 batch_size 大小的数据\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
