{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56cd64d-02f2-4aa8-af63-3a0e21129cb3",
   "metadata": {},
   "source": [
    "# 1. 模型（model）构建\n",
    "\n",
    "**1) 以实现一个 LeNet 为例介绍模型构建过程：**\n",
    "\n",
    "模型构建分为两个步骤：构建网络层和拼接网络层\n",
    "\n",
    "在下面 lenet.py 的代码中，首先在 `__init__()` 中构建网络层，然后在 `forward()` 前向传播函数中拼接网络层。通常，一个模型（model）必须有一个 `forward()` 前向传播函数。\n",
    "\n",
    "```\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    # 在 __init__() 中构建网络层\n",
    "    def __init__(self, classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, classes)\n",
    "\n",
    "    # 在 forward() 前向传播函数中拼接网络层\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "**2) nn.Module**\n",
    "\n",
    "**所有的模型和网络层都是继承于 nn.Module 这个类**\n",
    "\n",
    "**torch.nn 有 4 个主要部分：**\n",
    "\n",
    "- nn.Parameter：张量子类，存储为张量的形式，同样也具有张量的 8 个基本属性（例如可求取梯度），表示可学习参数，如权重、偏置等\n",
    "- nn.Module：所有网络层基类，管理网络属性\n",
    "- nn.functional：包含函数的具体实现，如卷积、池化、激活函数等\n",
    "- nn.init：提供丰富的参数初始化方法\n",
    "\n",
    "**nn.Module 有 8 个重要的属性，这 8 个属性都是以 `OrderedDict()` 有序字典的形式存放其取值，属性 parameters 和 modules 比较重要：**\n",
    "\n",
    "- parameters：存储管理 nn.Parameter 类\n",
    "- modules：存储管理 nn.Module 类\n",
    "- buffers：存储管理缓冲属性\n",
    "- `***hooks`：存储管理钩子函数，`***` 代表有 5 个类似的属性\n",
    "    \n",
    "LeNet 继承于 nn.Module，所以 LeNet 类的实例具有 nn.Module 的 8 个重要属性，其中 modules 属性会在其有序字典中会存放前面构建网络层部分构建的网络层，这些网络层在构建时同样也是继承于 nn.Module，所以这些网络层也具有 nn.Module 的 8 个重要属性。即：一个模型（model）可以包含多个子模型（model），这些模型（model）和子模型（model）都有 8 个有序字典管理其 8 个属性。\n",
    "\n",
    "**在前面模型构建中，需要先在 `__init__()` 中初始化 nn.Module 父类的 `__init__()`，实现 nn.Module 属性的初始化，即生成 8 个空的有序字典管理模型（model）的属性：**\n",
    "\n",
    "```\n",
    "class LeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "    ···\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "# 2. 模型容器 Containers\n",
    "\n",
    "**1) nn.Sequential()**\n",
    "\n",
    "**nn.Sequential() 是网络层容器，用于按顺序包装多个网络层。**\n",
    "\n",
    "以实现一个 LeNet 为例介绍 nn.Sequential：在 LeNet 中，以全连接层为边界，使用 nn.Sequential() 将前面的卷积层和池化层包装为特征提取模块(features)，使用 nn.Sequential() 将后面的全连接层包装为分类器模块(classifier)，然后在前向传播函数中将两个模块连接起来，特征提取模块提取特征，分类器模块输出分类结果。\n",
    "\n",
    "```\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LeNetSequential(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # 使用 nn.Sequetial() 将卷积层和池化层包装为特征提取模块，并赋给实例属性 features\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),)\n",
    "\n",
    "        # 使用 nn.Sequetial() 将 3 个全连接层包装为分类器模块，并赋给实例属性 classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, classes),)\n",
    "\n",
    "    # 在前向传播函数中将两个模块连接起来\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 实例化模型，并使用示例数据运行模型的前向传播函数\n",
    "net = LeNetSequential(classes=2)\n",
    "example_img = torch.randn((4, 3, 32, 32), dtype=torch.float32)\n",
    "output = net(example_img)\n",
    "print(net)\n",
    "print(output)\n",
    "```\n",
    "\n",
    "**在将网络层包装成模块时，还可以引入有序字典 OrderDict，以备在需要的时候实现对模块内网络层的索引：**\n",
    "\n",
    "```\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class LeNetSequentialOrderDict(nn.Module):\n",
    "    def __init__(self, classes):\n",
    "        super(LeNetSequentialOrderDict, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(OrderedDict({\n",
    "            'conv1': nn.Conv2d(3, 6, 5),\n",
    "            'relu1': nn.ReLU(inplace=True),\n",
    "            'pool1': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            'conv2': nn.Conv2d(6, 16, 5),\n",
    "            'relu2': nn.ReLU(inplace=True),\n",
    "            'pool2': nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        }))\n",
    "\n",
    "        self.classifier = nn.Sequential(OrderedDict({\n",
    "            'fc1': nn.Linear(16*5*5, 120),\n",
    "            'relu3': nn.ReLU(),\n",
    "\n",
    "            'fc2': nn.Linear(120, 84),\n",
    "            'relu4': nn.ReLU(inplace=True),\n",
    "\n",
    "            'fc3': nn.Linear(84, classes),\n",
    "        }))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**2) nn.ModuleList()**\n",
    "\n",
    "**nn.ModuleList() 是网络层容器，用于包装多个网络层，可以以迭代的方式创建多个网络层，也可以索引网络层，类似于 python 的 list。**\n",
    "\n",
    "主要方法：\n",
    "\n",
    "- append()：在 ModuleList 后面添加网络层\n",
    "- extend()：拼接两个 ModuleList\n",
    "- insert()：在 ModuleList 中指定位置插入网络层\n",
    "\n",
    "```\n",
    "class ModuleList(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleList, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(20)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**3) nn.ModuleDict()**\n",
    "\n",
    "**nn.ModuleDict 像 python 的 dict 一样包装多个网络层，以索引的方法调用网络层。**\n",
    "\n",
    "主要方法：\n",
    "\n",
    "- clear()：清空 ModuleDict\n",
    "- items()：返回可迭代的键值对\n",
    "- keys()：返回字典的键\n",
    "- values()：返回字典的值\n",
    "- pop()：从字典中弹出一对键值，该对儿键值从字典中删除\n",
    "\n",
    "```\n",
    "class ModuleDict(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModuleDict, self).__init__()\n",
    "        self.choices = nn.ModuleDict({\n",
    "            'conv': nn.Conv2d(10, 10, 3),\n",
    "            'pool': nn.MaxPool2d(3)\n",
    "        })\n",
    "\n",
    "        self.activations = nn.ModuleDict({\n",
    "            'relu': nn.ReLU(),\n",
    "            'prelu': nn.PReLU()\n",
    "        })\n",
    "\n",
    "    def forward(self, x, choice, act):\n",
    "        x = self.choices[choice](x)\n",
    "        x = self.activations[act](x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "**4) 总结**\n",
    "\n",
    "- nn.Sequential：顺序性，各网络层之间严格按照顺序，常用于模块构建，内部包含一个 for 循环前向传播机制，用于模块内网络层的前向传播\n",
    "- nn.ModuleList：迭代性，常用于大量重复网络构建，通过 for 循环实现重复构建\n",
    "- nn.ModuleDict：索引性，常用于可选择的网络层的构建\n",
    "\n",
    "<br/>\n",
    "\n",
    "# 3. 模型介绍：AlexNet\n",
    "\n",
    "**AlexNet 在 2012 年开创了卷积神经网络的新时代，AlexNet 的特点如下：**\n",
    "\n",
    "1. 采用 ReLU 激活函数替换 sigmoid 等饱和激活函数，减轻梯度消失。\n",
    "2. 采用 LRN（local response normalization，局部响应值归一化）对数据归一化，减轻梯度消失。现在常用的是 batch normalization。\n",
    "3. Dropout：提高全连接层的鲁棒性，增加网络的泛化能力。\n",
    "4. Data Augmentation\n",
    "\n",
    "<br/>\n",
    "\n",
    "# 4. nn 网络层\n",
    "\n",
    "**1) nn.Conv2d()**\n",
    "\n",
    "`nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
    "\n",
    "函数功能：对多个二维信号进行二维卷积\n",
    "\n",
    "主要参数：\n",
    "\n",
    "- in_channels：输入通道数\n",
    "- out_channels：输出通道数，等价于卷积核个数\n",
    "- kernel_size：卷积核尺寸\n",
    "- stride：步长\n",
    "- padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变\n",
    "- dilation：卷积核扩张幅度，默认为 1，1 代表没有扩张；卷积核扩张主要是为了扩大视野\n",
    "- groups：分组卷积设置，主要是为了模型的轻量化\n",
    "- bias：偏置\n",
    "\n",
    "**2) nn.MaxPool2d()**\n",
    "\n",
    "`nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)`\n",
    "\n",
    "函数功能：进行 2 维的最大值池化\n",
    "\n",
    "主要参数：\n",
    "\n",
    "- kernel_size：池化核尺寸\n",
    "- stride：步长，通常与 kernel_size 一致，确保池化时不重叠\n",
    "- padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变\n",
    "- dilation：池化核扩张幅度，默认为 1，1 代表没有扩张\n",
    "- ceil_mode：默认为 False，尺寸向下取整；为 True 时，尺寸向上取整\n",
    "- return_indices：为 True 时，返回最大值池化所使用的像素的索引，这些记录的索引通常在反最大值池化时使用，把像素放在对应的位置上\n",
    "\n",
    "**3) nn.AvgPool2d()**\n",
    "\n",
    "`torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)`\n",
    "\n",
    "函数功能：进行 2 维的平均池化\n",
    "\n",
    "主要参数：\n",
    "\n",
    "- kernel_size：池化核尺寸\n",
    "- stride：步长，通常与 kernel_size 一致，确保池化时不重叠\n",
    "- padding：填充宽度，主要是为了调整输出的特征图大小，一般把 padding 设置合适的值后，保持输入和输出的图像尺寸不变\n",
    "- dilation：池化核扩张幅度，默认为 1，1 代表没有扩张\n",
    "- ceil_mode：默认为 False，尺寸向下取整；为 True 时，尺寸向上取整\n",
    "- count_include_pad：在计算平均值时，是否把 padding 填充值考虑在内计算\n",
    "- divisor_override：除法因子。在计算平均值时，分子是像素值的总和，分母默认是像素值的个数；如果设置了 divisor_override，把分母改为 divisor_override\n",
    "\n",
    "**4) nn.Linear()**\n",
    "\n",
    "`nn.Linear(in_features, out_features, bias=True)`\n",
    "\n",
    "函数功能：线性层又称为全连接层，其每个神经元与上一个层所有神经元相连，实现对前一层的线性组合或线性变换。\n",
    "\n",
    "主要参数：\n",
    "\n",
    "- in_features：输入结点数\n",
    "- out_features：输出结点数\n",
    "- bias：是否需要偏置\n",
    "\n",
    "**5) nn.Sigmoid()**\n",
    "\n",
    "激活函数层\n",
    "\n",
    "计算公式：$y=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "导数/梯度公式：$y^{\\prime}={y}*{(1-y)}$\n",
    "\n",
    "特性：\n",
    "\n",
    "- 输出值在(0,1)，符合概率\n",
    "- 导数/梯度范围是 `[0, 0.25]`，容易导致梯度消失\n",
    "- 输出为非 0 均值，破坏数据分布\n",
    "\n",
    "**6) nn.tanh()**\n",
    "\n",
    "激活函数层\n",
    "\n",
    "计算公式：$y=\\frac{\\sin x}{\\cos x}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$\n",
    "\n",
    "导数/梯度公式：$y^{\\prime}=1-y^{2}$\n",
    "\n",
    "特性：\n",
    "\n",
    "- 输出值在(-1, 1)，数据符合 0 均值\n",
    "- 导数/梯度范围是 (0,1)，梯度消失麻烦比 Sigmoid 小一些，但依然容易导致梯度消失\n",
    "\n",
    "**7) nn.ReLU()**\n",
    "\n",
    "激活函数层，修正线性单元\n",
    "\n",
    "计算公式：$y=max(0, x)$，在正半轴为 $y=x$，在负半轴恒为 0\n",
    "\n",
    "导数/梯度公式：$y^{\\prime}=1(x>0), undefined(x=0), 0(x<0)$\n",
    "\n",
    "特性：\n",
    "\n",
    "- 输出值均为正数，负半轴的导数为 0，容易导致死神经元\n",
    "- 导数/梯度是 1，缓解梯度消失，但容易引发梯度爆炸\n",
    "\n",
    "**针对 RuLU 负半轴的导数为 0，会导致死神经元的缺点，介绍 3 种改进的 RuLU 激活函数：**\n",
    "\n",
    "- nn.LeakyReLU()：负半轴在第三象限有一个很小的倾斜，有一个参数 negative_slope 设置负半轴斜率\n",
    "- nn.PReLU()：有一个参数 init 设置初始斜率，这个斜率是可学习的\n",
    "- nn.RReLU()：R 是 random 的意思，负半轴斜率每次都是随机取 `[lower, upper]` 之间的一个数\n",
    "\n",
    "<br/>\n",
    "\n",
    "**梯度消失**：由于计算导数的链式法则，小于 1 的导数在多次相乘后得到的值越来越小，直至接近于 0，这就是导数消失/梯度消失\n",
    "\n",
    "**梯度爆炸**：由于计算导数的链式法则，大于 1 的导数在多次相乘后得到的值越来越大，这就是导数爆炸/梯度爆炸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
