{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f03fea-8b54-47c4-90a9-b172a5fc0305",
   "metadata": {},
   "source": [
    "Pytorch 先动态搭建计算图，然后运行反向传播进行自动求导，下面就介绍一下 Pytorch 的计算图和自动求导机制：\n",
    "\n",
    "## 1. 计算图\n",
    "计算图是用来描述运算的有向无环图，有两个主要元素：结点和边，结点表示张量数据，边表示运算。\n",
    "\n",
    "计算图采用链式法则进行微分求导。\n",
    "\n",
    "计算图示例：y = (x+w) * (w+1)\n",
    "\n",
    "![](./图1.png)\n",
    "\n",
    "用代码实现该计算图示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75d748bd-889a-4462-bdd2-9c9464e62244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)   \n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccc63e8-d501-429e-a6f7-85647c4c4220",
   "metadata": {},
   "source": [
    "# 2. tensor 四个和计算图相关的属性\n",
    "tensor 有 8 个属性，分别是 data, dtype, shape, device, requires_grad, grad, grad_fn, is_leaf，下面结合代码介绍后四个和计算图相关的属性："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12c7fb6-1a30-460e-914f-d72d79ceba66",
   "metadata": {},
   "source": [
    "## 1) requires_grad：指示张量是否需要计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aafe90e8-2fe1-49b3-bb71-63f81f3adb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, True, True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看张量是否需要计算梯度\n",
    "w.requires_grad, x.requires_grad, a.requires_grad, b.requires_grad, y.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83df867a-dcae-49e9-9da0-25a51a336e4c",
   "metadata": {},
   "source": [
    "也许大家会有疑问，在创建 a, b, y 时，并没有设置 requires_grad=True，为啥上面代码他们的输出结果也都为 True 呢？因为 a, b, y 都是依赖于叶子结点的结点，而依赖于依赖于叶子结点的结点的 requires_grad 属性默认值为 True。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72b80b-87b1-4968-a155-072eeaf208f1",
   "metadata": {},
   "source": [
    "## 2) is_leaf：指示张量是否为叶子结点\n",
    "\n",
    "叶子结点：用户创建的张量称为叶子结点，是整个计算图的根基，如第一张图的 x 和 w；在反向传播中，所有梯度的计算都依赖于叶子结点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d8f7c99-09a0-498a-af2f-cee220292806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False, False, False)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看张量是否是叶子结点\n",
    "w.is_leaf, x.is_leaf, a.is_leaf, b.is_leaf, y.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7b8c9-8c44-44da-9113-bd26636c9e36",
   "metadata": {},
   "source": [
    "## 3) grad：指示张量的梯度值\n",
    "\n",
    "注意：在求解梯度的反向传播结束之后，非叶子结点的梯度占用的内存会被释放掉，叶子结点的梯度占用的内存会被保留。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0005adac-b988-45d0-acd2-ef6464fabde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24606\\AppData\\Local\\Temp\\ipykernel_8600\\2984833489.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  w.grad, x.grad, a.grad, b.grad, y.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([5.]), tensor([2.]), None, None, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看张量的梯度\n",
    "w.grad, x.grad, a.grad, b.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ac914-d218-4cf4-a0ae-11b13b82510b",
   "metadata": {},
   "source": [
    "在上面代码运行结果中，由于 a, b, y 的梯度占用的内存已经被释放掉，所以 a, b, y 的梯度显示为 None。\n",
    "如果想保留某些非叶子结点的梯度，而不是在反向传播之后释放掉其占用的内存，可以使用 retain_grad() 设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6544b735-bf76-4c33-98f2-e207feecdf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24606\\AppData\\Local\\Temp\\ipykernel_8600\\289658673.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  w.grad, x.grad, a.grad, b.grad, y.grad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([10.]), tensor([4.]), tensor([2.]), None, None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retain_grad() 使用示例\n",
    "# 保留非叶子结点 a 的梯度\n",
    "a = torch.add(w, x) \n",
    "a.retain_grad()\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward()\n",
    "# print(w.grad)\n",
    "\n",
    "# 查看张量的梯度\n",
    "w.grad, x.grad, a.grad, b.grad, y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ac6f2d-4414-43a2-8ff3-678276ff115e",
   "metadata": {},
   "source": [
    "在上面代码运行结果中，非叶子结点 a 的梯度被保留了。\n",
    "\n",
    "### 3-1) 梯度清零问题\n",
    "\n",
    "在上面代码的运行结果中，我们发现 w.grad 和 x.grad 的值从 5 和 2 变成了 10 和 4，原因在于 autograd 求解的叶子结点的梯度占用的内存会被保留，在多次反向传播过程中会逐渐累加，不会自动清零。\n",
    "\n",
    "但是需要注意的是，对于非叶子结点，即使设置了 a.retain_grad()，在多次反向传播过程中，其梯度值并不会逐渐累加，一直都是同一个值。\n",
    "\n",
    "如果我们使用完叶子结点的梯度之后想将叶子结点的梯度清零，可以设置 w.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744fd608-eec0-4530-bb9d-78bb8eb68048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.]) tensor([4.])\n",
      "3147837759792 3147837759632\n",
      "tensor([0.]) tensor([0.])\n",
      "3147837759792 3147837759632\n"
     ]
    }
   ],
   "source": [
    "print(w.grad, x.grad)\n",
    "print(id(w), id(x))\n",
    "\n",
    "w.grad.zero_()\n",
    "x.grad.zero_()\n",
    "print(w.grad, x.grad)\n",
    "print(id(w), id(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226e447-954a-424a-b2a1-10f34f3aebd6",
   "metadata": {},
   "source": [
    "上面梯度清零操作用到的 w.grad.zero_()，函数后为啥紧跟一个下划线呢？\n",
    "\n",
    "### 3-2) 原位操作\n",
    "函数后紧跟一个下划线表示原位（in-place）操作，即在原内存地址对数据进行修改。\n",
    "\n",
    "下面通过代码解释原位操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dd15fe8-5407-43d9-9daf-e7e0356edfbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.]) 3146323276400\n",
      "tensor([2.]) 3146323276400\n",
      "tensor([3.]) 3146323276400\n",
      "tensor([4.]) 3147837005392\n"
     ]
    }
   ],
   "source": [
    "m = torch.ones(1,)\n",
    "print(m, id(m))\n",
    "\n",
    "# 运算 1 \n",
    "m.add_(1)\n",
    "print(m, id(m))\n",
    "\n",
    "# 运算 2\n",
    "m += 1\n",
    "print(m, id(m))\n",
    "\n",
    "# 运算 3\n",
    "m = m + 1\n",
    "print(m, id(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab65b9a-558d-4312-ba37-6592bd993ad9",
   "metadata": {},
   "source": [
    "我们可以发现，前 2 种运算结果的内存地址都是和初始地址一样的，两者都是原位操作；\n",
    "\n",
    "注意：+= 和 先+后赋值不是等同的，后者创建了新的内存地址。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0733b8ce-d3b8-4c4a-9e23-931120c18700",
   "metadata": {},
   "source": [
    "## 4) grad_fn：记录张量创建时所用的方法（函数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404ae165-2a33-40e0-93d9-2b4672d2805d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " None,\n",
       " <AddBackward0 at 0x2dc8f7af7c0>,\n",
       " <AddBackward0 at 0x2dc8f7af1c0>,\n",
       " <MulBackward0 at 0x2dc8f7af3d0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看张量创建时所用的方法（函数）\n",
    "w.grad_fn, x.grad_fn, a.grad_fn, b.grad_fn, y.grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f9fac7-c5c2-40e3-ac8f-9d00806f6b4d",
   "metadata": {},
   "source": [
    "w 和 x 是我们直接定义的，没有经过任何方法/函数去创建，所以它俩的 grad_fn 为 None。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d4dee-f02f-4dea-8fde-c1993c43be5c",
   "metadata": {},
   "source": [
    "# 3. 动态图和静态图\n",
    "\n",
    "动态图：边运算边搭建计算图，优点是灵活；\n",
    "\n",
    "静态图：先搭建计算图，后运算，优点是高效；\n",
    "\n",
    "Pytorch 的计算图采用的是动态图机制；\n",
    "\n",
    "TensorFlow 的计算图采用的是静态图机制。\n",
    "\n",
    "示例：运行以下代码的过程就是搭建计算图的过程，运行一次代码就会创建一次计算图。\n",
    "\n",
    "    w = torch.tensor([1.], requires_grad=True)\n",
    "    x = torch.tensor([2.], requires_grad=True)\n",
    "        \n",
    "    a = torch.add(w, x)   \n",
    "    b = torch.add(w, 1)\n",
    "    y = torch.mul(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ef97b9-7151-46c7-b6e1-cb7aeec7d0f4",
   "metadata": {},
   "source": [
    "# 4. torch.autograd.backward() 自动求导\n",
    "\n",
    "xxx.backward() 和 torch.autograd.backward() 是等价的，都是用于自动求取梯度，有 gradient, retain_graph, create_graph, inputs 四个参数：\n",
    "\n",
    "inputs：用于求导的张量\n",
    "\n",
    "create_graph：参考 torch.autograd.grad() 中的介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe9427-cd8f-421d-a96b-a66fa614b755",
   "metadata": {},
   "source": [
    "## 1) retain_graph：保存计算图\n",
    "\n",
    "执行完反向传播之后，计算图将被释放，再次执行 y.backward() 将会报错，如果想再次执行 y.backward()，可以将 retain_graph 设置为 True，retain_graph 默认为 False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce4af60-31d4-4bf9-a03b-06c60d394b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.])\n",
      "tensor([5.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.add(w, x) \n",
    "a.retain_grad()\n",
    "b = torch.add(w, 1)\n",
    "y = torch.mul(a, b)\n",
    "\n",
    "y.backward(retain_graph=True)\n",
    "print(w.grad)\n",
    "w.grad.zero_()\n",
    "\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdd473-b03d-47b2-aaae-240fad72c1d2",
   "metadata": {},
   "source": [
    "## 2) gardient：设置多个梯度之间的权重\n",
    "\n",
    "例如，以下示例中，w 的梯度值 = y0 对 w 的梯度值 * 权重 + y1 对 w 的梯度值 * 权重 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90a1bc33-cd1f-4d3b-8860-68c386c53ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.])\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor([1.], requires_grad=True)\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "a = torch.add(w, x)    \n",
    "b = torch.add(w, 1)\n",
    "\n",
    "y0 = torch.mul(a, b)   \n",
    "y1 = torch.add(a, b) \n",
    "\n",
    "loss = torch.cat([y0, y1], dim=0)     \n",
    "grad_tensors = torch.tensor([1., 2.])  # 创建多个梯度的权重张量\n",
    "\n",
    "loss.backward(gradient=grad_tensors)  \n",
    "\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941734d-de2e-4e88-a969-83f2802553ce",
   "metadata": {},
   "source": [
    "# 5. torch.autograd.grad() 自动求导\n",
    "\n",
    "torch.autograd.grad() 用来针对性地求取某个受关注张量的梯度，有 outputs, inputs, retain_graph, create_graph, grad_outputs 五个参数；\n",
    "\n",
    "outputs：用于求导的张量\n",
    "\n",
    "inputs：需要梯度的张量\n",
    "\n",
    "retain_graph：参考 torch.autograd.backward() 中的介绍\n",
    "\n",
    "grad_outputs：参考 torch.autograd.backward() 中对 gardient 的介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb8e68d-7e29-4809-9c03-a24e6f3ea444",
   "metadata": {},
   "source": [
    "## 1) create_graph：创建导数的计算图\n",
    "create_graph 设置为 True 时可以创建导数的计算图，导数的计算图用来对导数进行求导（高阶求导）。\n",
    "\n",
    "代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "214dd5ff-15dd-4f8f-b53f-c0121b08fa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([6.], grad_fn=<MulBackward0>),)\n",
      "(tensor([2.]),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3.], requires_grad=True)\n",
    "y = torch.pow(x, 2)\n",
    "\n",
    "grad_1 = torch.autograd.grad(y, x, create_graph=True)  # grad_1 = dy/dx = 2x = 2 * 3 = 6\n",
    "print(grad_1)  # 返回一个元组\n",
    "\n",
    "# 对导数进行求导\n",
    "grad_2 = torch.autograd.grad(grad_1[0], x)  # grad_2 = d(dy/dx)/dx = d(2x)/dx = 2\n",
    "print(grad_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
