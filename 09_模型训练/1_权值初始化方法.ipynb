{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd506da-d3a0-403a-bca0-c79a56b1fa74",
   "metadata": {},
   "source": [
    "在搭建好网络模型之后，需要对网络模型中的权值进行初始化。适当的权值初始化可以加快模型的收敛，而不恰当的权值初始化可能引发梯度消失或者梯度爆炸，最终导致模型无法训练。\n",
    "\n",
    "# 1. 梯度消失与梯度爆炸\n",
    "\n",
    "![](图1.png)\n",
    "\n",
    "根据链式求导法则推出的结果可以看到，${W}_{2}$ 的梯度依赖于前一层的输出 $H_{1}$，因此：\n",
    "\n",
    "- 如果$H_{1}$ 趋近于零，那么 ${W}_{2}$ 的梯度也接近于 0，造成梯度消失；\n",
    "\n",
    "- 如果$H_{1}$ 趋近于无穷大，那么 ${W}_{2}$ 的梯度也接近于无穷大，造成梯度爆炸；\n",
    "\n",
    "**要避免梯度爆炸或者梯度消失，就要严格控制网络层输出数值的范围，各个网络层输出数值的范围保持在一个固定的范围内，我们可以使用方差/标准差来描述数值的变化范围，通常使各个网络层输出数值的方差在 1 附近（0 均值）。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3a59d-c5b3-4c4d-b9ee-70fe124445f5",
   "metadata": {},
   "source": [
    "# 2. 仅包含全连接层时，全连接层权值的初始化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b553b0-6629-4996-8e96-14cfcdb20cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:15.959932327270508\n",
      "layer:1, std:256.6237487792969\n",
      "layer:2, std:4107.24560546875\n",
      "layer:3, std:65576.8125\n",
      "layer:4, std:1045011.875\n",
      "layer:5, std:17110408.0\n",
      "layer:6, std:275461408.0\n",
      "layer:7, std:4402537984.0\n",
      "layer:8, std:71323615232.0\n",
      "layer:9, std:1148104736768.0\n",
      "layer:10, std:17911758454784.0\n",
      "layer:11, std:283574846619648.0\n",
      "layer:12, std:4480599809064960.0\n",
      "layer:13, std:7.196814275405414e+16\n",
      "layer:14, std:1.1507761512626258e+18\n",
      "layer:15, std:1.853110740188555e+19\n",
      "layer:16, std:2.9677725826641455e+20\n",
      "layer:17, std:4.780376223769898e+21\n",
      "layer:18, std:7.613223480799065e+22\n",
      "layer:19, std:1.2092652108825478e+24\n",
      "layer:20, std:1.923257075956356e+25\n",
      "layer:21, std:3.134467063655912e+26\n",
      "layer:22, std:5.014437766285408e+27\n",
      "layer:23, std:8.066615144249704e+28\n",
      "layer:24, std:1.2392661553516338e+30\n",
      "layer:25, std:1.9455688099759845e+31\n",
      "layer:26, std:3.0238180658999113e+32\n",
      "layer:27, std:4.950357571077011e+33\n",
      "layer:28, std:8.150925520353362e+34\n",
      "layer:29, std:1.322983152787379e+36\n",
      "layer:30, std:nan\n",
      "output is nan in 30 layers\n",
      "tensor([[ 9.8052e+36,  2.5191e+37,  8.4358e+36,  ...,  4.1859e+37,\n",
      "         -2.3880e+37, -1.1118e+37],\n",
      "        [-3.6358e+37,  4.5755e+35, -2.7716e+36,  ..., -1.8793e+37,\n",
      "          4.1133e+36, -1.2764e+37],\n",
      "        [ 2.1538e+37, -3.1103e+37,  2.5804e+37,  ...,  6.9849e+36,\n",
      "          3.2139e+37,  4.8494e+36],\n",
      "        ...,\n",
      "        [ 1.3798e+37,  7.6824e+36, -2.9655e+36,  ...,  8.7788e+35,\n",
      "          1.3106e+37,  6.6469e+36],\n",
      "        [ 1.2969e+37,  2.3706e+37, -1.0296e+37,  ...,  1.5095e+37,\n",
      "         -3.8905e+37, -1.1750e+37],\n",
      "        [-8.2960e+36, -8.1296e+36, -7.4200e+36,  ..., -1.9674e+37,\n",
      "         -1.5635e+37,  1.5640e+36]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)  # 权值初始化为标准正态分布，mean=0, std=1\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 100\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f58d12a-efd0-4e51-9a08-d5ca2a5380d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "上面这个网络模型运行前向传播之后，我们可以看到每一层的标准差越来越大，并在在 30 层时超出了数据可以表示的范围。\n",
    "\n",
    "<br/>\n",
    "\n",
    "下面推导为什么网络层输出的标准差越来越大：\n",
    "\n",
    "**3 个基础公式：**\n",
    "\n",
    "$E(X \\times Y)=E(X) \\times E(Y)$：两个相互独立的随机变量的乘积的期望等于它们的期望的乘积。\n",
    "\n",
    "$D(X)=E(X^{2}) - [E(X)]^{2}$：一个随机变量的方差等于它的平方的期望减去期望的平方\n",
    "\n",
    "$D(X+Y)=D(X)+D(Y)$：两个相互独立的随机变量之和的方差等于它们的方差的和。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**根据上述 3 个基础公式，可以推导出两个相互独立的随机变量的乘积的方差如下：**\n",
    "\n",
    "$D(X \\times Y)=E[(XY)^{2}] - [E(XY)]^{2}=D(X) \\times D(Y) + D(X) \\times [E(Y)]^{2} + D(Y) \\times [E(X)]^{2}$\n",
    "\n",
    "如果$E(X)=0$，$E(Y)=0$，那么$D(X \\times Y)=D(X) \\times D(Y)$\n",
    "\n",
    "<br/>\n",
    "\n",
    "**将该推导结果应用到神经元节点上：**\n",
    "\n",
    "![](图2.png)\n",
    "\n",
    "我们以第一个隐藏层的第一个神经元 $\\mathrm{H}_{11}$ 为例，X 是输入层的输入值，W 是由输入层的输入值的权值，n 是输入层神经元的数量，输入值 X 和权值 W 都是服从 $N(0,1)$ 的正态分布，所以第一个隐藏层的第一个神经元的输出值的方差为：$\\begin{aligned} \\mathbf{D}\\left(\\mathrm{H}_{11}\\right) &=n \\end{aligned}$，标准差为：$\\operatorname{std}\\left(\\mathrm{H}_{11}\\right)=\\sqrt{n}$。\n",
    "\n",
    "进一步可知，第一个隐藏层的输出值的方差也为 n，标准差也为 $\\sqrt{n}$。\n",
    "\n",
    "输入层的输入值的方差为 1，所以由输入层的输入值到第一个隐藏层的输出值，方差扩大了 n 倍，标准差扩大了 $\\sqrt{n}$ 倍。\n",
    "\n",
    "由不严谨但正确的类推可知，**若权值 W 的标准差都是 1，则每经过一个网络层，网络层的输出值的方差就会扩大 n 倍，标准差就会扩大$\\sqrt{n}$倍，n 为每层神经元个数**。此结论也可以由上面代码输出的每个网络层的输出值得到验证，每层神经元的数量为 256，网络层的输出值的标准差逐层扩大约 16 倍。\n",
    "\n",
    "如果每经过一个网络层，网络层的输出值的方差就会扩大 n 倍，这样经过数十个网络层之后，网络层的输出值的方差就会大到计算机无法表示，因此我们需要采取措施避免这种情况发生。**本网络层输出值的方差与上一网络层的神经元个数、上一网络层的输出值（亦即本网络层的输入值）的方差、上一网络层输出值的权值的方差有关**，这其中比较好改变的是上一网络层输出值（亦即本网络层的输入值）的权值的方差 $D(W)$，**使 $D(W)= \\frac{1}{n}$，$std(W)=\\sqrt\\frac{1}{n}$，可以确保每经过一个网络层，网络层输出值的方差大致不变**。\n",
    "\n",
    "<br/>\n",
    "\n",
    "下面通过代码展示：将权值 W 的标准差设置为 $\\sqrt\\frac{1}{n}$ 对前向传播过程中每个网络层输出值的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ca0d423-100c-46da-a882-4c9b6df426dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:1.0022112131118774\n",
      "layer:1, std:1.0033732652664185\n",
      "layer:2, std:1.0147582292556763\n",
      "layer:3, std:1.0430957078933716\n",
      "layer:4, std:1.0486743450164795\n",
      "layer:5, std:1.0437393188476562\n",
      "layer:6, std:1.0411100387573242\n",
      "layer:7, std:1.0687036514282227\n",
      "layer:8, std:1.0842468738555908\n",
      "layer:9, std:1.0959652662277222\n",
      "layer:10, std:1.0800222158432007\n",
      "layer:11, std:1.0889637470245361\n",
      "layer:12, std:1.099170207977295\n",
      "layer:13, std:1.0889158248901367\n",
      "layer:14, std:1.0736167430877686\n",
      "layer:15, std:1.0679516792297363\n",
      "layer:16, std:1.0509815216064453\n",
      "layer:17, std:1.0493961572647095\n",
      "layer:18, std:1.0645091533660889\n",
      "layer:19, std:1.0877567529678345\n",
      "tensor([[-0.9184, -0.9523, -0.1050,  ...,  0.5743, -0.8024,  1.1647],\n",
      "        [ 0.1230,  3.1904, -1.1976,  ..., -1.4894,  0.6069,  0.3317],\n",
      "        [-1.2829,  0.9495, -0.0994,  ..., -0.8504,  0.0426, -0.5808],\n",
      "        ...,\n",
      "        [ 0.4939,  1.2548, -0.6355,  ..., -0.9356,  0.6341, -0.2524],\n",
      "        [ 0.0402,  0.6965, -0.5138,  ..., -0.3765,  0.5294, -0.5700],\n",
      "        [-0.4540, -0.1427, -1.4548,  ...,  0.5571,  0.9215, -0.4376]],\n",
      "       grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_nums))  # 权值初始化为正态分布，mean=0, std=根号1/n\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814d4e42-309d-4b14-bcee-e74b6e0d871c",
   "metadata": {},
   "source": [
    "从上述代码的运行结果可以看到，通过将权值 W 的标准差设置为 $\\sqrt\\frac{1}{n}$，前向传播过程中每个网络层输出值的标准差都保持在 1 左右。\n",
    "\n",
    "<br/>\n",
    "\n",
    "上述前向传播是模型仅包含全连接层（线性变换），不包含激活函数层（非线性变换）时的实验结果，如果在 forward() 中添加激活函数层，每一层的输出值的方差会越来越小，会导致梯度消失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f311f0c1-a496-4d13-81ce-c5c6e7dec058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.627248227596283\n",
      "layer:1, std:0.48665833473205566\n",
      "layer:2, std:0.41344448924064636\n",
      "layer:3, std:0.3640071451663971\n",
      "layer:4, std:0.32617107033729553\n",
      "layer:5, std:0.2971877455711365\n",
      "layer:6, std:0.27194929122924805\n",
      "layer:7, std:0.2612219452857971\n",
      "layer:8, std:0.2483828216791153\n",
      "layer:9, std:0.23589369654655457\n",
      "layer:10, std:0.22312353551387787\n",
      "layer:11, std:0.21378853917121887\n",
      "layer:12, std:0.20496906340122223\n",
      "layer:13, std:0.19552716612815857\n",
      "layer:14, std:0.1851242482662201\n",
      "layer:15, std:0.1773529201745987\n",
      "layer:16, std:0.17073562741279602\n",
      "layer:17, std:0.16620661318302155\n",
      "layer:18, std:0.16274142265319824\n",
      "layer:19, std:0.1617128998041153\n",
      "tensor([[-0.1143, -0.1310, -0.0734,  ...,  0.1242, -0.1479,  0.1477],\n",
      "        [ 0.0461,  0.3603, -0.1624,  ..., -0.2285,  0.0689, -0.0195],\n",
      "        [-0.0896,  0.0675,  0.0440,  ..., -0.1235, -0.0068, -0.0136],\n",
      "        ...,\n",
      "        [ 0.0708,  0.2205, -0.0834,  ..., -0.1684,  0.1373,  0.0136],\n",
      "        [ 0.0098,  0.0694, -0.0805,  ..., -0.0150,  0.1509, -0.0791],\n",
      "        [-0.0627,  0.0137, -0.2227,  ...,  0.0118,  0.0841,  0.0233]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.tanh(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_nums))  # 权值初始化为正态分布，mean=0, std=根号1/n\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca5b71-2704-4b58-baa5-8078dda47769",
   "metadata": {},
   "source": [
    "通过上述代码输出的每一网络层的输出值可知，在添加激活函数层之后，每一层输出值的标准差越来越小，随着网络层加深，可能会导致梯度消失。\n",
    "\n",
    "为了解决这个问题，出现了 Xavier 初始化方法（针对饱和激活函数，如 sigmoid 和 tanh）与 Kaiming 初始化方法（针对非饱和激活函数，如 relu）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8976881-51c4-49af-b18c-85c4b1e6fe88",
   "metadata": {},
   "source": [
    "# 2. 包含全连接层和饱和激活函数层时，全连接层权值的初始化：Xavier 权值初始化方法\n",
    "\n",
    "目标：保持网络层输出值的方差在恰当范围，通常在 1 左右\n",
    "\n",
    "适用对象：主要针对饱和激活函数，如 sigmoid 和 tanh\n",
    "\n",
    "<br/>\n",
    "\n",
    "若保持网络层输出值的方差在恰当范围，且同时考虑前向传播和反向传播，需要满足两个等式：\n",
    "\n",
    "$\\boldsymbol{n}_{\\boldsymbol{i}} * \\boldsymbol{D}(\\boldsymbol{W})=\\mathbf{1}$ \n",
    "\n",
    "$\\boldsymbol{n}_{\\boldsymbol{i+1}} * \\boldsymbol{D}(\\boldsymbol{W})=\\mathbf{1}$\n",
    "\n",
    "可得：$D(W)=\\frac{2}{n_{i}+n_{i+1}}$。\n",
    "\n",
    "<br/>\n",
    "\n",
    "若使 Xavier 方法初始化的权值服从均匀分布 $U[-a, a]$，那么方差 $D(W)=\\frac{(-a-a)^{2}}{12}=\\frac{(2 a)^{2}}{12}=\\frac{a^{2}}{3}$;\n",
    "\n",
    "令$\\frac{2}{n_{i}+n_{i+1}}=\\frac{a^{2}}{3}$，解得：$\\boldsymbol{a}=\\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}$；\n",
    "\n",
    "**所以 Xavier 方法将权值  $W$  初始化为均匀分布 $U\\left[-\\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}\\right]$ 时，可以保持网络层输出值的方差在恰当范围**（同时考虑了前向传播和反向传播）。\n",
    "\n",
    "代码实验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69cf433c-92b9-4c2c-a710-416c80b0594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.7564043998718262\n",
      "layer:1, std:0.6912741661071777\n",
      "layer:2, std:0.6716636419296265\n",
      "layer:3, std:0.6640926599502563\n",
      "layer:4, std:0.650257408618927\n",
      "layer:5, std:0.6443925499916077\n",
      "layer:6, std:0.6554051041603088\n",
      "layer:7, std:0.6477739214897156\n",
      "layer:8, std:0.6523249745368958\n",
      "layer:9, std:0.6515239477157593\n",
      "layer:10, std:0.6521815061569214\n",
      "layer:11, std:0.6556642055511475\n",
      "layer:12, std:0.6526561379432678\n",
      "layer:13, std:0.6564072370529175\n",
      "layer:14, std:0.6549282670021057\n",
      "layer:15, std:0.6534885168075562\n",
      "layer:16, std:0.6481824517250061\n",
      "layer:17, std:0.6542978882789612\n",
      "layer:18, std:0.656050443649292\n",
      "layer:19, std:0.6504389047622681\n",
      "tensor([[ 0.9328,  0.9638,  0.2703,  ..., -0.3082,  0.4021,  0.8018],\n",
      "        [-0.5733,  0.6387,  0.7284,  ...,  0.4264,  0.7715,  0.0212],\n",
      "        [-0.2264,  0.5689, -0.8231,  ..., -0.9744,  0.6164, -0.1837],\n",
      "        ...,\n",
      "        [-0.9853, -0.7947, -0.1297,  ...,  0.4011,  0.6222, -0.8152],\n",
      "        [-0.9399, -0.4636, -0.2226,  ...,  0.3696, -0.4646,  0.5930],\n",
      "        [-0.8704, -0.0527, -0.8197,  ...,  0.4081, -0.3404, -0.9398]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.tanh(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                a = np.sqrt(6 / (self.neural_nums + self.neural_nums))\n",
    "                tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                a *= tanh_gain  # 上下限乘以 tanh 激活函数对输入值标准差的缩小比例\n",
    "                nn.init.uniform_(m.weight.data, -a, a)  # 权值初始化为 [-a, a] 的均匀分布\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4d1178-6137-4a05-893e-110c060f0107",
   "metadata": {},
   "source": [
    "在添加了饱和激活函数层之后，通过将权值初始化为 `[-a, a]` 的均匀分布，以及上下限乘以 tanh 激活函数对输入值标准差的缩小比例，我们可以看到，前向传播过程每个网络层的输出值的标准差保持在 0.65 左右，解决了每一层输出值的标准差越来越小的问题。\n",
    "\n",
    "<br/>\n",
    "\n",
    "PyTorch 也提供了 Xavier 初始化方法，可以直接调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636d4bfa-d7b7-408b-b68c-d37749dbd331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.7564043998718262\n",
      "layer:1, std:0.6912741661071777\n",
      "layer:2, std:0.6716636419296265\n",
      "layer:3, std:0.6640926599502563\n",
      "layer:4, std:0.650257408618927\n",
      "layer:5, std:0.6443925499916077\n",
      "layer:6, std:0.6554051041603088\n",
      "layer:7, std:0.6477739214897156\n",
      "layer:8, std:0.6523249745368958\n",
      "layer:9, std:0.6515239477157593\n",
      "layer:10, std:0.6521815061569214\n",
      "layer:11, std:0.6556642055511475\n",
      "layer:12, std:0.6526561379432678\n",
      "layer:13, std:0.6564072370529175\n",
      "layer:14, std:0.6549282670021057\n",
      "layer:15, std:0.6534885168075562\n",
      "layer:16, std:0.6481824517250061\n",
      "layer:17, std:0.6542978882789612\n",
      "layer:18, std:0.656050443649292\n",
      "layer:19, std:0.6504389047622681\n",
      "tensor([[ 0.9328,  0.9638,  0.2703,  ..., -0.3082,  0.4021,  0.8018],\n",
      "        [-0.5733,  0.6387,  0.7284,  ...,  0.4264,  0.7715,  0.0212],\n",
      "        [-0.2264,  0.5689, -0.8231,  ..., -0.9744,  0.6164, -0.1837],\n",
      "        ...,\n",
      "        [-0.9853, -0.7947, -0.1297,  ...,  0.4011,  0.6222, -0.8152],\n",
      "        [-0.9399, -0.4636, -0.2226,  ...,  0.3696, -0.4646,  0.5930],\n",
      "        [-0.8704, -0.0527, -0.8197,  ...,  0.4081, -0.3404, -0.9398]],\n",
      "       grad_fn=<TanhBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.tanh(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                tanh_gain = nn.init.calculate_gain('tanh')\n",
    "                nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)  # 直接调用 PyTorch 提供的 Xavier 初始化方法\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5de286-51e7-4c2f-b9c6-cb6daf5eef82",
   "metadata": {},
   "source": [
    "# 3. 包含全连接层和非饱和激活函数层时，全连接层权值的初始化：Kaiming 权值初始化方法\n",
    "\n",
    "若激活函数是非饱和激活函数（如 Relu），Xavier 方法将不再适用，2015 年针对 ReLU 及其变种等激活函数提出了 Kaiming 初始化方法。\n",
    "\n",
    "目标：保持网络层输出值的方差在恰当范围，通常在 1 左右\n",
    "\n",
    "适用对象：Relu 及其变种\n",
    "\n",
    "<br/>\n",
    "\n",
    "若保持网络层的输出值在一个适当的范围，需要满足：（证明略）\n",
    "\n",
    "$\\mathrm{D}(W)=\\frac{2}{n_{i}}$（针对 ReLU）\n",
    "\n",
    "$\\mathrm{D}(W)=\\frac{2}{\\left(1+a^{2}\\right) n_{i}}$，$\\operatorname{std}(W)=\\sqrt{\\frac{2}{\\left(1+a^{2}\\right) * n_{i}}}$（针对 ReLu 的变种，a 表示负半轴的斜率，$n_{i}$ 为输入层/上一层神经元的数量）\n",
    "\n",
    "**所以 Kaiming 方法将权值  $W$  初始化为正态分布 $N(0, std(w))$ 时，可以保持网络层输出值的方差在恰当范围。**\n",
    "\n",
    "代码实验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "827ea857-0848-4861-8ea9-a81d6156cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.8343690633773804\n",
      "layer:1, std:0.8857383728027344\n",
      "layer:2, std:0.8768216371536255\n",
      "layer:3, std:0.8620700836181641\n",
      "layer:4, std:0.8756188750267029\n",
      "layer:5, std:0.8784897923469543\n",
      "layer:6, std:0.8935869336128235\n",
      "layer:7, std:0.8712248206138611\n",
      "layer:8, std:0.9630030989646912\n",
      "layer:9, std:0.9609673619270325\n",
      "layer:10, std:0.8539234399795532\n",
      "layer:11, std:0.8073139190673828\n",
      "layer:12, std:0.9279561638832092\n",
      "layer:13, std:0.9541431069374084\n",
      "layer:14, std:0.9491678476333618\n",
      "layer:15, std:1.0219509601593018\n",
      "layer:16, std:0.8949427604675293\n",
      "layer:17, std:0.916878342628479\n",
      "layer:18, std:0.8786735534667969\n",
      "layer:19, std:0.9656010270118713\n",
      "tensor([[0.0000, 0.1963, 1.8828,  ..., 0.0000, 0.0000, 1.6750],\n",
      "        [0.0000, 0.0000, 1.3597,  ..., 0.0000, 0.0000, 1.2228],\n",
      "        [0.0000, 0.0000, 2.4619,  ..., 0.0000, 0.0000, 2.1729],\n",
      "        ...,\n",
      "        [0.0000, 0.3190, 0.8542,  ..., 0.0000, 0.0000, 1.6953],\n",
      "        [0.0000, 0.0305, 1.7972,  ..., 0.0000, 0.0000, 1.9260],\n",
      "        [0.0000, 0.0000, 1.7365,  ..., 0.0000, 0.0000, 2.1252]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data, std=np.sqrt(2 / self.neural_nums))  # 使用 Kaiming 方法初始化权值\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6451258-2c47-46f6-84ee-62bf88739011",
   "metadata": {},
   "source": [
    "在添加了非饱和激活函数层 Relu 之后，**通过将权值初始化为标准差为 $\\sqrt{\\frac{2}{n_{i}}}$ 的正态分布，我们可以看到，前向传播过程每个网络层的输出值的标准差保持在一个恰当的范围。**\n",
    "\n",
    "<br/>\n",
    "\n",
    "PyTorch 也提供了 Kaiming 初始化方法，可以直接调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be9f1397-9090-421a-bf13-cafbabfb882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.8343690633773804\n",
      "layer:1, std:0.8857383728027344\n",
      "layer:2, std:0.8768216371536255\n",
      "layer:3, std:0.8620700836181641\n",
      "layer:4, std:0.8756188750267029\n",
      "layer:5, std:0.8784897923469543\n",
      "layer:6, std:0.8935869336128235\n",
      "layer:7, std:0.8712248206138611\n",
      "layer:8, std:0.9630030989646912\n",
      "layer:9, std:0.9609673619270325\n",
      "layer:10, std:0.8539234399795532\n",
      "layer:11, std:0.8073139190673828\n",
      "layer:12, std:0.9279561638832092\n",
      "layer:13, std:0.9541431069374084\n",
      "layer:14, std:0.9491678476333618\n",
      "layer:15, std:1.0219509601593018\n",
      "layer:16, std:0.8949427604675293\n",
      "layer:17, std:0.916878342628479\n",
      "layer:18, std:0.8786735534667969\n",
      "layer:19, std:0.9656010270118713\n",
      "tensor([[0.0000, 0.1963, 1.8828,  ..., 0.0000, 0.0000, 1.6750],\n",
      "        [0.0000, 0.0000, 1.3597,  ..., 0.0000, 0.0000, 1.2228],\n",
      "        [0.0000, 0.0000, 2.4619,  ..., 0.0000, 0.0000, 2.1729],\n",
      "        ...,\n",
      "        [0.0000, 0.3190, 0.8542,  ..., 0.0000, 0.0000, 1.6953],\n",
      "        [0.0000, 0.0305, 1.7972,  ..., 0.0000, 0.0000, 1.9260],\n",
      "        [0.0000, 0.0000, 1.7365,  ..., 0.0000, 0.0000, 2.1252]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)  # 直接调用 PyTorch 提供的 Kaiming 初始化方法\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffc5e0-a2e0-4212-b3ee-1a19e53d817f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. 总结\n",
    "\n",
    "**1. 仅包含全连接层时，全连接层权值的初始化**\n",
    "\n",
    "将权值 W 的标准差设置为 $\\sqrt\\frac{1}{n}$\n",
    "```\n",
    "def initialize(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight.data, std=np.sqrt(1/self.neural_nums))  # 权值初始化为正态分布，mean=0, std=根号1/n\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "***\n",
    "\n",
    "**2. 包含全连接层和饱和激活函数层时，全连接层权值的初始化：Xavier 权值初始化方法**\n",
    "\n",
    "饱和激活函数：sigmoid 和 tanh 等\n",
    "\n",
    "<br/>\n",
    "\n",
    "若保持网络层输出值的方差在恰当范围，且同时考虑前向传播和反向传播，需要满足两个等式：\n",
    "\n",
    "$\\boldsymbol{n}_{\\boldsymbol{i}} * \\boldsymbol{D}(\\boldsymbol{W})=\\mathbf{1}$ （$n_i$ 为第 i 层神经元的数量）\n",
    "\n",
    "$\\boldsymbol{n}_{\\boldsymbol{i+1}} * \\boldsymbol{D}(\\boldsymbol{W})=\\mathbf{1}$\n",
    "\n",
    "可得：$D(W)=\\frac{2}{n_{i}+n_{i+1}}$。\n",
    "\n",
    "<br/>\n",
    "\n",
    "若使 Xavier 方法初始化的权值服从均匀分布 $U[-a, a]$，那么方差 $D(W)=\\frac{(-a-a)^{2}}{12}=\\frac{(2 a)^{2}}{12}=\\frac{a^{2}}{3}$;\n",
    "\n",
    "令$\\frac{2}{n_{i}+n_{i+1}}=\\frac{a^{2}}{3}$，解得：$\\boldsymbol{a}=\\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}$；\n",
    "\n",
    "<br/>\n",
    "\n",
    "**所以 Xavier 方法将权值  $W$  初始化为均匀分布 $U\\left[-\\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{i}+n_{i+1}}}\\right]$ 时，可以保持网络层输出值的方差在恰当范围**（同时考虑了前向传播和反向传播）。\n",
    "```\n",
    "def initialize(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            a = np.sqrt(6 / (self.neural_nums + self.neural_nums))\n",
    "            tanh_gain = nn.init.calculate_gain('tanh')\n",
    "            a *= tanh_gain  # 上下限乘以 tanh 激活函数对输入值标准差的缩小比例\n",
    "            nn.init.uniform_(m.weight.data, -a, a)  # 权值初始化为 [-a, a] 的均匀分布\n",
    "\n",
    "\n",
    "def initialize(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            tanh_gain = nn.init.calculate_gain('tanh')\n",
    "            nn.init.xavier_uniform_(m.weight.data, gain=tanh_gain)  # 直接调用 PyTorch 提供的 Xavier 初始化方法\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "***\n",
    "\n",
    "**3. 包含全连接层和非饱和激活函数层时，全连接层权值的初始化：Kaiming 权值初始化方法**\n",
    "\n",
    "非饱和激活函数：relu 等\n",
    "\n",
    "<br/>\n",
    "\n",
    "若保持网络层的输出值在一个适当的范围，需要满足：（证明略）\n",
    "\n",
    "$\\mathrm{D}(W)=\\frac{2}{n_{i}}$（针对 ReLU）\n",
    "\n",
    "$\\mathrm{D}(W)=\\frac{2}{\\left(1+a^{2}\\right) n_{i}}$，$\\operatorname{std}(W)=\\sqrt{\\frac{2}{\\left(1+a^{2}\\right) * n_{i}}}$（针对 ReLu 的变种，a 表示负半轴的斜率，$n_{i}$ 为输入层/上一层神经元的数量）\n",
    "\n",
    "<br/>\n",
    "\n",
    "**所以 Kaiming 方法将权值  $W$  初始化为正态分布 $N(0, std(w))$ 时，可以保持网络层输出值的方差在恰当范围。**\n",
    "```\n",
    "def initialize(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight.data, std=np.sqrt(2 / self.neural_nums))  # 使用 Kaiming 方法初始化权值\n",
    "                \n",
    "def initialize(self):\n",
    "    for m in self.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_normal_(m.weight.data)  # 直接调用 PyTorch 提供的 Kaiming 初始化方法\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c409608-a8ed-4edf-a7a8-8da1cee7eda4",
   "metadata": {},
   "source": [
    "# 5. PyTorch 中提供的 10 中初始化方法\n",
    "\n",
    "- Xavier 均匀分布\n",
    "- Xavier 正态分布\n",
    "- Kaiming 均匀分布\n",
    "- Kaiming 正态分布\n",
    "- 均匀分布\n",
    "- 正态分布\n",
    "- 常数分布\n",
    "- 正交矩阵初始化\n",
    "- 单位矩阵初始化\n",
    "- 稀疏矩阵初始化\n",
    "\n",
    "**权值初始化原则**：保持每一层输出值的方差一致，约为 1，即方差一致性原则；每一层的输出值不能太大，也不能太小。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbde1ba6-5475-4e33-9592-7de7da016369",
   "metadata": {},
   "source": [
    "# 6. nn.init.calculate_gain()\n",
    "\n",
    "主要功能：计算激活函数的标准差变化尺度，即输入数据的标准差除以经过激活函数之后输出数据的标准差，即标准差的缩放比例。\n",
    "\n",
    "在 Xavier 权值初始化方法中使用了 `tanh_gain = nn.init.calculate_gain('tanh')`\n",
    "\n",
    "主要参数：\n",
    "- nonlinearity：激活函数名称\n",
    "- param：激活函数的参数，如 Leaky ReLU 的 negative_slop（负半轴斜率）。\n",
    "\n",
    "代码实验："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8088b41a-ae17-47d5-a596-bef7b5369b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tanh_gain1: tensor(1.5899)\n",
      "tanh_gain2: 1.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(10000) \n",
    "out = torch.tanh(x)\n",
    "\n",
    "tanh_gain1 = x.std() / out.std() \n",
    "print('tanh_gain1:', tanh_gain1)\n",
    "\n",
    "tanh_gain2 = nn.init.calculate_gain('tanh') \n",
    "print('tanh_gain2:', tanh_gain2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c9162-32f4-4dd5-9963-da4beeac2e2c",
   "metadata": {},
   "source": [
    "结果显示：对于服从标准正态分布的输入值而言，tanh 激活函数会使输入值的标准差缩小 1.6 倍左右。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
