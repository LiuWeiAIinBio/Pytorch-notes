{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8847f8e5-1270-4f3e-9850-c2cf0470c53a",
   "metadata": {},
   "source": [
    "# 1. optimizer 的基本情况\n",
    "\n",
    "PyTorch 中的优化器是用于**管理并更新**模型中可学习参数的值，使得模型输出值更加接近真实标签值。\n",
    "\n",
    "PyTorch 中提供了 Optimizer 类，定义如下：\n",
    "```\n",
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):        \n",
    "        self.defaults = defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []\n",
    "```\n",
    "主要属性：\n",
    "- defaults：优化器的超参数\n",
    "- state：参数的缓存，如 momentum 中需要用到前几次的梯度，就缓存在这个变量中\n",
    "- param_groups：管理的参数组，是一个 list，参数可以划分为不同组，每组参数都是单独的字典，list 可以包含多个字典，实现对参数的差异化训练\n",
    "- _step_count：记录更新次数，在学习率调整中使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05981d0-63ef-4b8c-81e3-83891234a938",
   "metadata": {},
   "source": [
    "# 2. optimizer 的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6076a9-5f73-4648-b722-9021743f3d0b",
   "metadata": {},
   "source": [
    "## 2.1 step()\n",
    "\n",
    "功能：执行一步梯度更新，执行 `optimizer.step()` 更新梯度，也就是 `weight.data` 减去梯度乘以学习率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b7c1cc8-15f5-4670-ae16-45e22d5d8358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight before step:tensor([[-0.5174, -0.9269],\n",
      "        [ 0.9223,  0.6238]])\n",
      "weight after step:tensor([[-1.5174, -1.9269],\n",
      "        [-0.0777, -0.3762]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "weight.grad = torch.ones((2, 2))  # 梯度设为 1\n",
    "\n",
    "optimizer = torch.optim.SGD([weight], lr=1)\n",
    "print(\"weight before step:{}\".format(weight.data))\n",
    "optimizer.step()\n",
    "print(\"weight after step:{}\\n\".format(weight.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f96039c2-f597-48a3-baf2-77b7252a9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight in optimizer:2288419532288\n",
      "weight in weight:2288419532288\n"
     ]
    }
   ],
   "source": [
    "print(\"weight in optimizer:{}\\nweight in weight:{}\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bff09a4-cbf7-4427-b84c-8de03dd3e715",
   "metadata": {},
   "source": [
    "可以看到优化器的 param_groups 中存储的参数和 weight 的内存地址是一样的，所以优化器中保存的是参数的地址索引，而不是把参数复制到优化器中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb368f-47f4-4862-a26c-23c5f3cec10e",
   "metadata": {},
   "source": [
    "## 2.2 zero_grad()\n",
    "\n",
    "功能：清空所管理参数的梯度。由于 PyTorch 的特性是张量的梯度不自动清零，因此每次反向传播之后都需要清空梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6cb4fd-de8d-41fc-af3a-82d6767f237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight.grad is \n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "after optimizer.zero_grad(), weight.grad is None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"weight.grad is \\n{}\".format(weight.grad))\n",
    "optimizer.zero_grad()\n",
    "print(\"after optimizer.zero_grad(), weight.grad is {}\\n\".format(weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170aa46e-d2ae-4a42-bd2b-0f88605f4863",
   "metadata": {},
   "source": [
    "## 2.3 add_param_group()\n",
    "\n",
    "功能：在 param_groups 这个 list 后面添加（append）参数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f3abe1-c7c4-41c8-866c-d26b68f40b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimizer.param_groups is\n",
      "[{'params': [tensor([[-1.5174, -1.9269],\n",
      "        [-0.0777, -0.3762]], requires_grad=True)], 'lr': 1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}]\n",
      "\n",
      "optimizer.param_groups is\n",
      "[{'params': [tensor([[-1.5174, -1.9269],\n",
      "        [-0.0777, -0.3762]], requires_grad=True)], 'lr': 1, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}, {'params': [tensor([[-0.8968, -0.7357, -1.8949],\n",
      "        [-0.8327,  1.5896, -1.0585],\n",
      "        [ 1.0978, -0.6106,  0.2272]], requires_grad=True)], 'lr': 0.0001, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None}]\n"
     ]
    }
   ],
   "source": [
    "print(\"optimizer.param_groups is\\n{}\\n\".format(optimizer.param_groups))\n",
    "\n",
    "w2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": w2, 'lr': 0.0001})\n",
    "\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797ef12-a96c-42f9-842a-2c20ba7bb7c7",
   "metadata": {},
   "source": [
    "## 2.4 state_dict()\n",
    "\n",
    "功能：获取优化器当前状态信息字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4696713e-8ec3-42fe-be07-2f5ae42380e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict:\n",
      " {'state': {}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0]}]}\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "print(\"state_dict:\\n\", optimizer.state_dict())\n",
    "\n",
    "# 使用 torch.save() 把 state_dict 保存到 pkl 文件中\n",
    "torch.save(optimizer.state_dict(), \"./optimizer_state_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfefcfdd-fe21-4f07-87d6-0adbfafe749c",
   "metadata": {},
   "source": [
    "## 2.5 load_state_dict()\n",
    "\n",
    "功能：加载保存的优化器状态信息字典，主要用于模型的断点续训练。我们可以在每隔一定数量的 epoch 就保存模型的 state_dict 到硬盘，在意外终止训练时，可以继续加载上次保存的状态，继续训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c606d1-793f-4955-ab65-123c66798565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict after load state:\n",
      " {'state': {}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0]}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24606\\AppData\\Local\\Temp\\ipykernel_20500\\887406690.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"./optimizer_state_dict.pkl\")\n"
     ]
    }
   ],
   "source": [
    "# 保存了 state_dict 之后，使用 torch.load() 把 state_dict 加载到内存中\n",
    "state_dict = torch.load(\"./optimizer_state_dict.pkl\")\n",
    "\n",
    "# 重新构建优化器，并使用 load_state_dict() 将保存的 state_dict 加载到模型中\n",
    "optimizer = torch.optim.SGD([weight], lr=0.2, momentum=0.9)  # 这里设置的参数在加载后会被覆盖\n",
    "optimizer.load_state_dict(state_dict)\n",
    "print(\"state_dict after load state:\\n\", optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc80025-ef5f-47d6-94ac-3d72ff5fef0c",
   "metadata": {},
   "source": [
    "# 3. optimizer 的参数\n",
    "\n",
    "## 3.1 学习率\n",
    "\n",
    "初始化时，一般把学习率设置为比较小的数，如 0.01，0.001。\n",
    "\n",
    "## 3.2 momentum 动量\n",
    "\n",
    "momentum（动量、冲量）：结合当前梯度与上一次更新信息，用于当前更新。\n",
    "\n",
    "**常规的更新参数（梯度下降）：**\n",
    "\n",
    "$w_{i+1}=w_{i}-l r * g\\left(w_{i}\\right)$\n",
    "\n",
    "**在 PyTroch 中更新参数（梯度下降）时，不仅考虑当前的梯度，还会结合前面的梯度。参数的更新公式是：**\n",
    "\n",
    "$w_{i+1}=w_{i}-lr * v_{i}$\n",
    "\n",
    "$v_{i}=m * v_{i-1}+g\\left(w_{i}\\right)=···=m^3* v_{i-3}+m^2 * g\\left(w_{i-2}\\right)+m * g\\left(w_{i-1}\\right)+g\\left(w_{i}\\right)=···$ \n",
    "\n",
    "其中 $w_{i+1}$ 表示第 $i+1$ 次更新的参数，lr 表示学习率，$v_{i}$ 表示更新量，$m$ 表示 momentum 系数，$g(w_{i})$ 表示 $w_{i}$ 的梯度。\n",
    "\n",
    "momentum 系数 $m$ 可以理解为记忆周期系数，$m$ 越小，记忆周期越短，$m$ 越大，记忆周期越长。通常将 $m$ 设置为 0.9，那么 $\\frac{1}{1-m}=\\frac{1}{1-0.9}=10$，表示更关注最近 10 次参数更新时的梯度。\n",
    "\n",
    "## 3.3 weight_decay 权值衰减（L2 正则项）\n",
    "\n",
    "**正则项有 L1 和 L2 正则项两种：**\n",
    "- L1 正则项：$\\sum\\boldsymbol|{w}_{i}|$，使用 L1 正则项会产生稀疏参数值（有 0 值的出现）\n",
    "- L2 正则项：$\\sum\\boldsymbol{w}_{i}^{2}$， L2 正则项又被称为权值衰减(weight decay)\n",
    "\n",
    "\n",
    "# 4. PyTroch 提供的 10 种优化器\n",
    "\n",
    "**torch.optim.SGD(params, lr, momentum=0, dampening=0, weight_decay=0, nesterov=False)**\n",
    "\n",
    "随机梯度下降法\n",
    "\n",
    "主要参数：\n",
    "- params：管理的参数组，是一个 list，参数可以划分为不同组，每组参数都是单独的字典\n",
    "- lr：初始学习率\n",
    "- momentum：动量系数\n",
    "- weight_decay：L2 正则化系数\n",
    "- nesterov：是否采用 NAG 梯度下降方法，默认为 False，一般不采用\n",
    "\n",
    "**其他：**\n",
    "- optim.Adagrad：自适应学习率梯度下降法\n",
    "- optim.RMSprop：Adagrad 的改进\n",
    "- optim.Adadelta\n",
    "- optim.Adam：RMSProp 集合 Momentum，这个是目前最常用的优化器，因为它可以使用较大的初始学习率。\n",
    "- optim.Adamax：Adam 增加学习率上限\n",
    "- optim.SparseAdam：稀疏版的 Adam\n",
    "- optim.ASGD：随机平均梯度下降\n",
    "- optim.Rprop：弹性反向传播，这种优化器通常是在所有样本都一起训练，也就是 batchsize 为全部样本时使用。\n",
    "- optim.LBFGS：BFGS 在内存上的改进"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
