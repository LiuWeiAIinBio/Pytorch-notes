{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84093168-c836-4961-90c4-ea6da70a249a",
   "metadata": {},
   "source": [
    "# 为什么要进行 Normalization ?\n",
    "\n",
    "本网络层输出值的方差与上一网络层的神经元个数、上一网络层的输出值（亦即本网络层的输入值）的方差、上一网络层输出值的权值的方差有关，为了控制本网络层输出值的方差，我们可以通过权值初始化来控制上一网络层输出值的权值的方差（即 $D(W)$），除此之外，我们也可以通过 Normalization 来控制上一网络层的输出值（亦即本网络层的输入值）的方差（即 $D(X)$）。\n",
    "\n",
    "![](./图片1.png)\n",
    "\n",
    "# Batch Normalization\n",
    "\n",
    "**1. 批标准化 Batch Normalization：**\n",
    "- 批是指一批数据，通常为 mini-batch；\n",
    "- 标准化是处理后的数据服从 $N(0,1)$ 的标准正态分布。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**2. Batch Normalization 最初想要解决的问题**：在训练过程中，数据需要经过多层的网络，如果数据在前向传播的过程中，数据尺度发生了变化，可能会导致梯度爆炸或者梯度消失，从而导致模型无法训练。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**3. 批标准化带来的额外优点：**\n",
    "- 可以使用更大的学习率，加速模型收敛\n",
    "- 可以不用精心设计权值初始化（有了 BN 层就可以不用精心设计权值初始化，可以将权值直接初始化为标准正态分布）\n",
    "- 可以不用 dropout 或者较小的 dropout\n",
    "- 可以不用 L2 或者可以使用较小的 L2(weight decay)\n",
    "- 可以不用 LRN (local response normalization)\n",
    "\n",
    "<br/>\n",
    "\n",
    "**4. Batch Normalization 的运算过程：**\n",
    "\n",
    "**标准化**：$\\widehat{x}_{i} \\leftarrow \\frac{x{i}-\\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{B}^{2}+\\epsilon}}$，其中$\\epsilon$ 是一个调整项\n",
    "\n",
    "**affine transform(缩放和平移)**：${y}_{i} \\leftarrow \\gamma \\widehat{x}_{i}+\\beta \\equiv \\mathrm{B} \\mathrm{N}_{\\gamma,\\beta}\\left(x_{i}\\right)$，这个操作可以增强模型的 capacity，也就是让模型自己判断是否要对数据进行标准化，进行多大程度的标准化。如果$\\gamma= \\sqrt{\\sigma_{B}^{2}}$，$\\beta=\\mu_{\\mathcal{B}}$，那么就实现了${x}_{i} \\rightarrow {y}_{i}$ 的恒等映射。\n",
    "\n",
    "**观察 Batch Normalization 的运算原理**：\n",
    "\n",
    "我们可以通过下图看到，Batch Normalization 在每一个特征维度上计算所有样本（样本数量为 batch_size 大小）的 runninng_mean 均值、running_var 方差、weight 和 bias。 \n",
    "\n",
    "![](./图片2.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "**5. Batch Normalization 的主要属性：**\n",
    "- runninng_mean：均值\n",
    "- running_var：方差\n",
    "- weight：affine transform 中的 $\\gamma$\n",
    "- bias：affine transform 中的 $\\beta$\n",
    "\n",
    "weight 和 bias 是训练中可学习的参数\n",
    "\n",
    "**观察 weight 和 bias 的 shape**：\n",
    "\n",
    "weight 和 bias 的 shape 都等于特征的数量 num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4742e2c8-1e02-46b3-a974-4520c29118c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设输入数据的形状是 (batch_size, num_features, features_channels, features_length, features_width)，创建一个输入张量\n",
    "batch_size = 2\n",
    "num_features = 4\n",
    "features_channels = 3\n",
    "features_length = 2\n",
    "features_width = 2\n",
    "input_tensor = torch.randn(batch_size, num_features, features_channels, features_length, features_width)\n",
    "\n",
    "# 创建一个 BatchNorm3d 模块\n",
    "bn = nn.BatchNorm3d(num_features)\n",
    "\n",
    "# 应用 BatchNorm3d\n",
    "normalized_tensor = bn(input_tensor)\n",
    "\n",
    "print(bn.weight.shape)\n",
    "print(bn.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62e3ae-e5aa-4e98-8bd6-76ebe1293939",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "**6. 以 `nn.BatchNorm3d()` 为例，介绍参数：**\n",
    "\n",
    "`torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)`\n",
    "\n",
    "- num_features：一个样本的特征数量，等于上一个卷积层的输出通道数，这个参数最重要；**Batch Normalization 会在每一个特征维度上计算所有样本（样本数量为 batch_size 大小）的 runninng_mean 均值、running_var 方差、weight 和 bias 这四个参数。**\n",
    "- eps：在进行标准化操作时的分母修正项，默认值很小，是为了避免分母为 0。\n",
    "- momentum：估计当前均值和方差的加权系数。\n",
    "- affine：是否需要 affine transform，默认为 True。\n",
    "- track_running_stats：True 为训练状态，此时均值和方差会根据每个 mini-batch 加权计算。False 为测试状态，此时均值和方差会当前数据的统计值。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**7. runninng_mean 均值和 running_var 方差的计算：**\n",
    "\n",
    "1) 训练时：采用指数加权平均计算\n",
    "- runninng_mean = (1-momentum) * pre_runninng_mean + momentum * mean_t\n",
    "- running_var = (1-momentum) * pre_runninng_var + momentum * var_t\n",
    "\n",
    "momentum 为记忆周期系数，pre_runninng_mean 为上一批次的均值，mean_t 为当前批次的均值，pre_runninng_var 为上一批次的方差，mean_t 为当前批次的方差\n",
    "\n",
    "2) 测试时：当前数据的统计值\n",
    "\n",
    "<br/>\n",
    "\n",
    "**8. 在 PyTorch 中，有 3 个 Batch Normalization 类：**\n",
    "- `bn = nn.BatchNorm1d(num_features=num_features, momentum=momentum)`，输入数据的形状是 batch_size * num_features * features_shape\n",
    "- `bn = nn.BatchNorm2d(num_features=num_features, momentum=momentum)`，输入数据的形状是 batch_size * num_features * (features_length * features_width)\n",
    "- `bn = nn.BatchNorm3d(num_features=num_features, momentum=momentum)`，输入数据的形状是 batch_size * num_features * (features_channels * features_length * features_width)\n",
    "\n",
    "举例：\n",
    "- `batch_size = 3, num_features = 5, features_shape = (1)` 表示输入数据的形状为 $3*5*1$，表示一个 mini-batch 有 3 个样本，每个样本有 5 个特征，每个特征的维度是 1，那么就会计算 5 组【均值、方差、weight 和 bias】，分别对应每个特征维度。\n",
    "- `batch_size = 3, num_features = 3, features_shape = (2, 2)` 表示输入数据的形状为 $3*3*2*2$，表示一个 mini-batch 有 3 个样本，每个样本有 3 个特征，每个特征的维度是 $2*2$。\n",
    "- `batch_size = 3, num_features = 3, features_shape = (3, 2, 2)` 表示输入数据的形状为 $3*3*3*2*2$，表示一个 mini-batch 有 3 个样本，每个样本有 3 个特征，每个特征的维度是 $3*2*2$。\n",
    "\n",
    "<br/>\n",
    "\n",
    "**9. 应用：**\n",
    "\n",
    "**无 BN 层，使用 Kaiming 方法初始化权值：**\n",
    "\n",
    "**注意：Batch Normalization 层一般在激活函数层的前一层。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7282b58-4ea7-4b05-93b1-582a672e26f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.8343690633773804\n",
      "layer:1, std:0.8857383728027344\n",
      "layer:2, std:0.8768216371536255\n",
      "layer:3, std:0.8620700836181641\n",
      "layer:4, std:0.8756188750267029\n",
      "layer:5, std:0.8784897923469543\n",
      "layer:6, std:0.8935869336128235\n",
      "layer:7, std:0.8712248206138611\n",
      "layer:8, std:0.9630030989646912\n",
      "layer:9, std:0.9609673619270325\n",
      "layer:10, std:0.8539234399795532\n",
      "layer:11, std:0.8073139190673828\n",
      "layer:12, std:0.9279561638832092\n",
      "layer:13, std:0.9541431069374084\n",
      "layer:14, std:0.9491678476333618\n",
      "layer:15, std:1.0219509601593018\n",
      "layer:16, std:0.8949427604675293\n",
      "layer:17, std:0.916878342628479\n",
      "layer:18, std:0.8786735534667969\n",
      "layer:19, std:0.9656010270118713\n",
      "tensor([[0.0000, 0.1963, 1.8828,  ..., 0.0000, 0.0000, 1.6750],\n",
      "        [0.0000, 0.0000, 1.3597,  ..., 0.0000, 0.0000, 1.2228],\n",
      "        [0.0000, 0.0000, 2.4619,  ..., 0.0000, 0.0000, 2.1729],\n",
      "        ...,\n",
      "        [0.0000, 0.3190, 0.8542,  ..., 0.0000, 0.0000, 1.6953],\n",
      "        [0.0000, 0.0305, 1.7972,  ..., 0.0000, 0.0000, 1.9260],\n",
      "        [0.0000, 0.0000, 1.7365,  ..., 0.0000, 0.0000, 2.1252]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)  # 使用 Kaiming 方法初始化权值\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30775003-f50f-40f3-9c9e-4e94e04bcf31",
   "metadata": {},
   "source": [
    "**有 BN 层，将权值直接初始化为标准正态分布：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa4e0b5f-48a3-49b5-904a-25546a1d1084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.5778297185897827\n",
      "layer:1, std:0.5812498927116394\n",
      "layer:2, std:0.581561267375946\n",
      "layer:3, std:0.5803059339523315\n",
      "layer:4, std:0.589631974697113\n",
      "layer:5, std:0.5759917497634888\n",
      "layer:6, std:0.5750452280044556\n",
      "layer:7, std:0.5770913362503052\n",
      "layer:8, std:0.5815293788909912\n",
      "layer:9, std:0.5840634703636169\n",
      "layer:10, std:0.5836883187294006\n",
      "layer:11, std:0.5750855803489685\n",
      "layer:12, std:0.5757170915603638\n",
      "layer:13, std:0.5789809226989746\n",
      "layer:14, std:0.5793242454528809\n",
      "layer:15, std:0.5792165994644165\n",
      "layer:16, std:0.5756219029426575\n",
      "layer:17, std:0.5778035521507263\n",
      "layer:18, std:0.5831732153892517\n",
      "layer:19, std:0.5793583989143372\n",
      "tensor([[0.5744, 0.0000, 0.0567,  ..., 2.1843, 0.2327, 0.7803],\n",
      "        [0.6528, 0.0000, 0.0000,  ..., 0.0000, 0.3646, 0.2953],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.9208, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.9975, 0.0000, 0.4790,  ..., 0.0099, 0.0819, 0.0000],\n",
      "        [0.0000, 0.1138, 0.0000,  ..., 0.0000, 0.6335, 0.0000],\n",
      "        [1.2653, 0.0000, 2.4845,  ..., 0.0000, 0.4312, 0.4834]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.bn = nn.BatchNorm1d(num_features=neural_nums)\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = self.bn(x)\n",
    "            x = torch.relu(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9a4e24-cec3-4fea-95be-12825ae53ad1",
   "metadata": {},
   "source": [
    "**无 BN 层，将权值直接初始化为标准正态分布：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ea18300-d7fe-4ebd-97d1-58a38dfd99f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:9.43980884552002\n",
      "layer:1, std:113.37451934814453\n",
      "layer:2, std:1269.7734375\n",
      "layer:3, std:14124.1572265625\n",
      "layer:4, std:162308.0625\n",
      "layer:5, std:1842326.75\n",
      "layer:6, std:21201752.0\n",
      "layer:7, std:233867664.0\n",
      "layer:8, std:2924641280.0\n",
      "layer:9, std:33018591232.0\n",
      "layer:10, std:331950915584.0\n",
      "layer:11, std:3550604689408.0\n",
      "layer:12, std:46173452763136.0\n",
      "layer:13, std:537134884716544.0\n",
      "layer:14, std:6045300149977088.0\n",
      "layer:15, std:7.363933600376422e+16\n",
      "layer:16, std:7.29592073165996e+17\n",
      "layer:17, std:8.456711715864707e+18\n",
      "layer:18, std:9.16900778393102e+19\n",
      "layer:19, std:1.1399807629213187e+21\n",
      "tensor([[0.0000e+00, 2.3178e+20, 2.2228e+21,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.9774e+21],\n",
      "        [0.0000e+00, 0.0000e+00, 1.6053e+21,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         1.4436e+21],\n",
      "        [0.0000e+00, 0.0000e+00, 2.9065e+21,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         2.5653e+21],\n",
      "        ...,\n",
      "        [0.0000e+00, 3.7656e+20, 1.0084e+21,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         2.0014e+21],\n",
      "        [0.0000e+00, 3.5980e+19, 2.1217e+21,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         2.2738e+21],\n",
      "        [0.0000e+00, 0.0000e+00, 2.0501e+21,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         2.5089e+21]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = torch.relu(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight.data)\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b57cfa-dd54-4bff-9784-c3aca3577324",
   "metadata": {},
   "source": [
    "**有 BN 层，使用 Kaiming 方法初始化权值：**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41ed9e86-f4a5-4efd-8063-f335d96ed1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer:0, std:0.5778279304504395\n",
      "layer:1, std:0.5812448859214783\n",
      "layer:2, std:0.5815566182136536\n",
      "layer:3, std:0.58030104637146\n",
      "layer:4, std:0.5896270275115967\n",
      "layer:5, std:0.5759868025779724\n",
      "layer:6, std:0.5750400424003601\n",
      "layer:7, std:0.5770862102508545\n",
      "layer:8, std:0.5815243721008301\n",
      "layer:9, std:0.5840585827827454\n",
      "layer:10, std:0.5836827158927917\n",
      "layer:11, std:0.5750799179077148\n",
      "layer:12, std:0.5757117867469788\n",
      "layer:13, std:0.578975260257721\n",
      "layer:14, std:0.5793191194534302\n",
      "layer:15, std:0.5792108178138733\n",
      "layer:16, std:0.5756176710128784\n",
      "layer:17, std:0.5777992010116577\n",
      "layer:18, std:0.5831680297851562\n",
      "layer:19, std:0.5793556571006775\n",
      "tensor([[0.5735, 0.0000, 0.0570,  ..., 2.1840, 0.2318, 0.7803],\n",
      "        [0.6528, 0.0000, 0.0000,  ..., 0.0000, 0.3644, 0.2948],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.9206, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.9971, 0.0000, 0.4790,  ..., 0.0098, 0.0819, 0.0000],\n",
      "        [0.0000, 0.1136, 0.0000,  ..., 0.0000, 0.6338, 0.0000],\n",
      "        [1.2664, 0.0000, 2.4838,  ..., 0.0000, 0.4313, 0.4836]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# 创建网络模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, neural_nums, layer_nums):\n",
    "        super().__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(neural_nums, neural_nums, bias=False) for i in range(layer_nums)])   # 创建数个完全一样的全连接层\n",
    "        self.bn = nn.BatchNorm1d(num_features=neural_nums)\n",
    "        self.neural_nums = neural_nums\n",
    "\n",
    "    # 前向传播\n",
    "    def forward(self, x):\n",
    "        for (i, linear) in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            x = self.bn(x)\n",
    "            x = torch.relu(x)\n",
    "            print(\"layer:{}, std:{}\".format(i, x.std()))\n",
    "            if torch.isnan(x.std()):\n",
    "                print(\"output is nan in {} layers\".format(i))\n",
    "                break     \n",
    "        return x\n",
    "\n",
    "    # 权值初始化\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight.data)  # 使用 Kaiming 方法初始化权值\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# 实例化网络模型，运行前向传播\n",
    "layer_nums = 20\n",
    "neural_nums = 256\n",
    "batch_size = 16\n",
    "\n",
    "net = MLP(neural_nums, layer_nums)\n",
    "net.initialize()\n",
    "inputs = torch.randn((batch_size, neural_nums))  # 输入值初始化为标准正态分布，mean=0, std=1\n",
    "output = net(inputs)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45e506d-0a64-4932-9286-a23d028dac19",
   "metadata": {},
   "source": [
    "**总结：**\n",
    "- 无 BN 层，使用 Kaiming 方法初始化权值：输出值的标准差很小\n",
    "- 有 BN 层，将权值直接初始化为标准正态分布：输出值的标准差很小\n",
    "- 无 BN 层，将权值直接初始化为标准正态分布：输出值的标准差越来越大，数据尺度越来越大\n",
    "- 有 BN 层，使用 Kaiming 方法初始化权值：输出值的标准差很小\n",
    "\n",
    "**【添加 BN 层】和【使用 Kaiming 方法初始化权值】这两种方法使用其一就能很好地控制输出值的数据尺度；两者都用也可以；两者都不用时，输出值的数据尺度越来越大。**\n",
    "\n",
    "以上针对的是非饱和激活函数 relu，饱和激活函数使用 Xavier 方法，同理。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
